{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmkWPP7G743A",
        "outputId": "a67b3da1-2912-41a2-eea9-dd1f80bca2b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.9.0 torchmetrics-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 필요한 라이브러리 가져오기"
      ],
      "metadata": {
        "id": "gC7MaelC8vln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchmetrics import Accuracy\n",
        "from torchmetrics.aggregation import MeanMetric"
      ],
      "metadata": {
        "id": "zdn0ASuq78sM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Build Config"
      ],
      "metadata": {
        "id": "myHicvwZ80zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = 'FashionMNIST'\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "data_root ='data'\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "base_lr = 0.01\n",
        "momentum = 0.9\n",
        "checkpoint_dir = 'checkpoint'"
      ],
      "metadata": {
        "id": "CSHkhcEU8yJs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build directory\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "w18Xx8sE9TwU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Build Datasets"
      ],
      "metadata": {
        "id": "0M2Yw8YJ9YAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5, ), (0.5, ))\n",
        "])\n",
        "\n",
        "train_data = FashionMNIST(data_root, train=True, download=True, transform=train_transform)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5, ), (0.5, ))\n",
        "])\n",
        "\n",
        "val_data = FashionMNIST(data_root, train=False, download=True, transform=val_transform)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLwGA98R9WM-",
        "outputId": "dd4b352f-672c-4552-e7af-963290741f72"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:01<00:00, 13943814.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 273510.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:00<00:00, 4971451.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 13832336.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Build Model"
      ],
      "metadata": {
        "id": "zBbrP1tGABdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.layers = nn.ModuleList()\n",
        "    self.layers.append(nn.Sequential(\n",
        "        nn.Linear(28 * 28, 500),\n",
        "        nn.ReLU()\n",
        "    ))\n",
        "    self.layers.append(nn.Sequential(\n",
        "        nn.Linear(500, 500),\n",
        "        nn.ReLU()\n",
        "    ))\n",
        "    self.head = nn.Linear(500, 10)\n",
        "  def forward(self, x):\n",
        "    x = x.reshape((x.shape[0], -1))\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    x = self.head(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Hyk6_7oy-Ycg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMG45MMyAAhr",
        "outputId": "076527be-b5c4-489e-c746-507d86e1e668"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyModel(\n",
            "  (layers): ModuleList(\n",
            "    (0): Sequential(\n",
            "      (0): Linear(in_features=784, out_features=500, bias=True)\n",
            "      (1): ReLU()\n",
            "    )\n",
            "    (1): Sequential(\n",
            "      (0): Linear(in_features=500, out_features=500, bias=True)\n",
            "      (1): ReLU()\n",
            "    )\n",
            "  )\n",
            "  (head): Linear(in_features=500, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Build Optimizer, Scheduler, loss function, metric function"
      ],
      "metadata": {
        "id": "AASRQ3bmAX2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr = base_lr, momentum = momentum)\n",
        "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs * len(train_loader))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "metric_fn = Accuracy(task='multiclass', num_classes=10)\n",
        "metric_fn = metric_fn.to(device)"
      ],
      "metadata": {
        "id": "ECh2Io1jAJMy"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Define Train Loop"
      ],
      "metadata": {
        "id": "mll9ydO-AxgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(loader, model, optimizer, scheduler, loss_fn, metric_fn, device):\n",
        "  model.train()\n",
        "\n",
        "  loss_mean = MeanMetric()\n",
        "  metric_mean = MeanMetric()\n",
        "\n",
        "  for inputs, targets in loader:\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # Forward\n",
        "    outputs = model(inputs)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    metric = metric_fn(outputs, targets)\n",
        "\n",
        "    #Backward\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    #Update\n",
        "    loss_mean.update(loss.to('cpu'))\n",
        "    metric_mean.update(metric.to('cpu'))\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "      # Summarize statistics\n",
        "  summary = {'loss': loss_mean.compute(), 'metric': metric_mean.compute()}\n",
        "\n",
        "  return summary"
      ],
      "metadata": {
        "id": "5qhST8P8Awtp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Define Evaluate Loop"
      ],
      "metadata": {
        "id": "1u8J3x2RB84c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(loader, model, loss_fn, metric_fn, device):\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Create average meters to measure loss and accuracy\n",
        "    loss_mean = MeanMetric()\n",
        "    metric_mean = MeanMetric()\n",
        "\n",
        "    # Evalute model for one epoch\n",
        "    for inputs, targets in loader:\n",
        "        # Move data to device\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        metric = metric_fn(outputs, targets)\n",
        "\n",
        "        # Update statistics\n",
        "        loss_mean.update(loss.to('cpu'))\n",
        "        metric_mean.update(metric.to('cpu'))\n",
        "\n",
        "    # Summarize statistics\n",
        "    summary = {'loss': loss_mean.compute(), 'metric': metric_mean.compute()}\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "Yndcw2q6B4um"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Define Main Loop"
      ],
      "metadata": {
        "id": "sLP9_XwZCLZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main loop\n",
        "for epoch in range(epochs):\n",
        "    # train one epoch\n",
        "    train_summary = train(train_loader, model, optimizer, scheduler, loss_fn, metric_fn, device)\n",
        "\n",
        "    # evaluate one epoch\n",
        "    val_summary = evaluate(val_loader, model, loss_fn, metric_fn, device)\n",
        "\n",
        "    # print log\n",
        "    print((f'Epoch {epoch+1}: '\n",
        "           + f'Train Loss {train_summary[\"loss\"]:.04f}, '\n",
        "           + f'Train Accuracy {train_summary[\"metric\"]:.04f}, '\n",
        "           + f'Test Loss {val_summary[\"loss\"]:.04f}, '\n",
        "           + f'Test Accuracy {val_summary[\"metric\"]:.04f}'))\n",
        "\n",
        "    # save model\n",
        "    state_dict = {\n",
        "        'epoch': epoch + 1,\n",
        "        'model': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    checkpoint_path = f'{checkpoint_dir}/{title}_last.pth'\n",
        "    torch.save(state_dict, checkpoint_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-qhwXu8CKWd",
        "outputId": "72328032-0667-4ff7-a99b-cacc029fce47"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss 0.5490, Train Accuracy 0.8003, Test Loss 0.4235, Test Accuracy 0.8487\n",
            "Epoch 2: Train Loss 0.3742, Train Accuracy 0.8635, Test Loss 0.3751, Test Accuracy 0.8657\n",
            "Epoch 3: Train Loss 0.3246, Train Accuracy 0.8822, Test Loss 0.3642, Test Accuracy 0.8666\n",
            "Epoch 4: Train Loss 0.2928, Train Accuracy 0.8930, Test Loss 0.3425, Test Accuracy 0.8777\n",
            "Epoch 5: Train Loss 0.2747, Train Accuracy 0.8999, Test Loss 0.3378, Test Accuracy 0.8807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r2qBXoaRCQ-d"
      },
      "execution_count": 32,
      "outputs": []
    }
  ]
}